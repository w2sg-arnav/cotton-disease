{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Imports ---\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import label2rgb\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights, vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import wandb  # For experiment tracking\n",
    "import optuna  # For hyperparameter optimization (used later)\n",
    "\n",
    "import albumentations as A  # For image augmentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from pytorch_grad_cam import GradCAM  # For Grad-CAM (used later)\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# New imports for progressive resizing and attention visualization\n",
    "from torchvision.transforms.functional import resize\n",
    "from torchvision.utils import make_grid\n",
    "from einops import rearrange  # For tensor manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Configuration ---\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Dataset ---\n",
    "DATASET_ROOT = \"/home/w2sg-arnav/cotton-disease/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection\"\n",
    "ORIGINAL_DIR = os.path.join(DATASET_ROOT, \"Original Dataset\")\n",
    "AUGMENTED_DIR = os.path.join(DATASET_ROOT, \"Augmented Dataset\")\n",
    "\n",
    "CLASSES = [\n",
    "    \"Bacterial Blight\",\n",
    "    \"Curl Virus\",\n",
    "    \"Healthy Leaf\",\n",
    "    \"Herbicide Growth Damage\",\n",
    "    \"Leaf Hopper Jassids\",\n",
    "    \"Leaf Redding\",\n",
    "    \"Leaf Variegation\",\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "CLASS_MAP = {i: name for i, name in enumerate(CLASSES)}\n",
    "\n",
    "# --- Training ---\n",
    "IMAGE_SIZE = (224, 224)  # ViT expects 224x224 images  # Start with 128x128 for progressive resizing\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 6\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Model ---\n",
    "MODEL_NAME = \"vit_b_16\"\n",
    "PRETRAINED = True\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Progressive Resizing ---\n",
    "# --- Progressive Resizing ---\n",
    "PROGRESSIVE_SIZES = [(128, 128), (224, 224), (384, 384)]  # Progressive resizing steps\n",
    "CURRENT_SIZE_INDEX = 0  # Start with the smallest size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Data Loading Functions ---\n",
    "class CottonLeafDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, class_names=CLASSES):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names\n",
    "        self.image_paths, self.labels = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                print(f\"Warning: Directory not found: {class_dir}\")\n",
    "                continue\n",
    "\n",
    "            for image_path in glob.glob(os.path.join(class_dir, \"*.jpg\")) + glob.glob(\n",
    "                os.path.join(class_dir, \"*.jpeg\")) + glob.glob(os.path.join(class_dir, \"*.png\")):\n",
    "                image_paths.append(image_path)\n",
    "                labels.append(i)\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (IOError, FileNotFoundError) as e:\n",
    "            print(f\"Error loading image: {image_path} - {e}. Returning a blank image.\")\n",
    "            return torch.zeros((3, *IMAGE_SIZE)), torch.tensor(0)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            # Convert PIL image to NumPy array for Albumentations\n",
    "            image_np = np.array(image)\n",
    "            # Apply Albumentations transform with named argument\n",
    "            transformed = self.transform(image=image_np)\n",
    "            image = transformed[\"image\"]  # Extract the transformed image\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def create_data_splits(data_dir, val_size=0.2, test_size=0.2, random_state=42):\n",
    "    image_files, labels = [], []\n",
    "    for i, class_name in enumerate(CLASSES):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for file_path in glob.glob(os.path.join(class_path, \"*\")):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_files.append(file_path)\n",
    "                labels.append(i)\n",
    "\n",
    "    train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "        image_files, labels, test_size=(val_size + test_size), random_state=random_state, stratify=labels\n",
    "    )\n",
    "    val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "        temp_files, temp_labels, test_size=(test_size / (val_size + test_size)), random_state=random_state, stratify=temp_labels\n",
    "    )\n",
    "    return train_files, val_files, test_files, train_labels, val_labels, test_labels\n",
    "\n",
    "def save_data_splits(train_files, val_files, test_files, output_dir=\".\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    def _write_list_to_file(file_list, filename):\n",
    "        with open(os.path.join(output_dir, filename), \"w\") as f:\n",
    "            for item in file_list:\n",
    "                f.write(f\"{item}\\n\")\n",
    "\n",
    "    _write_list_to_file(train_files, \"train_files.txt\")\n",
    "    _write_list_to_file(val_files, \"val_files.txt\")\n",
    "    _write_list_to_file(test_files, \"test_files.txt\")\n",
    "    print(f\"Data splits saved to {output_dir}\")\n",
    "\n",
    "def load_data_splits(output_dir=\".\"):\n",
    "    def _read_list_from_file(filename):\n",
    "        with open(os.path.join(output_dir, filename), \"r\") as f:\n",
    "            return [line.strip() for line in f]\n",
    "\n",
    "    train_files = _read_list_from_file(\"train_files.txt\")\n",
    "    val_files = _read_list_from_file(\"val_files.txt\")\n",
    "    test_files = _read_list_from_file(\"test_files.txt\")\n",
    "\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "def get_transforms(image_size=PROGRESSIVE_SIZES[CURRENT_SIZE_INDEX], train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),  # Resize to current progressive size\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),  # Resize to current progressive size\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "def create_data_loaders(data_dir, train_transform, val_transform, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_files, val_files, test_files = load_data_splits()\n",
    "    train_labels = [CLASSES.index(filepath.split(os.sep)[-2]) for filepath in train_files]\n",
    "    val_labels = [CLASSES.index(filepath.split(os.sep)[-2]) for filepath in val_files]\n",
    "    test_labels = [CLASSES.index(filepath.split(os.sep)[-2]) for filepath in test_files]\n",
    "\n",
    "    train_dataset = CottonLeafDataset(data_dir, transform=train_transform)\n",
    "    train_dataset.image_paths = train_files\n",
    "    train_dataset.labels = train_labels\n",
    "\n",
    "    val_dataset = CottonLeafDataset(data_dir, transform=val_transform)\n",
    "    val_dataset.image_paths = val_files\n",
    "    val_dataset.labels = val_labels\n",
    "\n",
    "    test_dataset = CottonLeafDataset(data_dir, transform=val_transform)\n",
    "    test_dataset.image_paths = test_files\n",
    "    test_dataset.labels = test_labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Model Definition ---\n",
    "def get_model(model_name=MODEL_NAME, pretrained=PRETRAINED, num_classes=NUM_CLASSES):\n",
    "    if model_name == \"vit_b_16\":\n",
    "        weights = ViT_B_16_Weights.DEFAULT if pretrained else None\n",
    "        model = vit_b_16(weights=weights)\n",
    "        model.heads = nn.Linear(model.heads[0].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Training Loop ---\n",
    "def train_model(model, train_loader, val_loader, learning_rate=LEARNING_RATE, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Update image size for progressive resizing\n",
    "        global CURRENT_SIZE_INDEX\n",
    "        if epoch > 0 and epoch % (epochs // len(PROGRESSIVE_SIZES)) == 0:\n",
    "            CURRENT_SIZE_INDEX = min(CURRENT_SIZE_INDEX + 1, len(PROGRESSIVE_SIZES) - 1)\n",
    "            print(f\"Updating image size to {PROGRESSIVE_SIZES[CURRENT_SIZE_INDEX]}\")\n",
    "\n",
    "            # Update transforms with the new image size\n",
    "            train_transforms = get_transforms(image_size=PROGRESSIVE_SIZES[CURRENT_SIZE_INDEX], train=True)\n",
    "            val_transforms = get_transforms(image_size=PROGRESSIVE_SIZES[CURRENT_SIZE_INDEX], train=False)\n",
    "\n",
    "            # Recreate DataLoaders with the new transforms\n",
    "            train_loader, val_loader, _ = create_data_loaders(ORIGINAL_DIR, train_transforms, val_transforms, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Logging to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"image_size\": PROGRESSIVE_SIZES[CURRENT_SIZE_INDEX][0],  # Log current image size\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Image Size: {PROGRESSIVE_SIZES[CURRENT_SIZE_INDEX]}\")\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.7287, Train Acc: 75.43%, Val Loss: 0.4039, Val Acc: 85.01%, Image Size: (128, 128)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, learning_rate, epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 36\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m total_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Main Execution ---\n",
    "import wandb\n",
    "\n",
    "if wandb.run is None:\n",
    "    run = wandb.init(project=\"vit\", entity=\"w2sgarnav\", name=\"w2sgarnav-vit-phase2\", mode=\"offline\")\n",
    "\n",
    "# Get Transforms\n",
    "train_transforms = get_transforms(image_size=IMAGE_SIZE, train=True)\n",
    "val_transforms = get_transforms(image_size=IMAGE_SIZE, train=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader, _ = create_data_loaders(ORIGINAL_DIR, train_transforms, val_transforms, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "# Get Model\n",
    "model = get_model()\n",
    "\n",
    "# Train Model\n",
    "train_model(model, train_loader, val_loader)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
