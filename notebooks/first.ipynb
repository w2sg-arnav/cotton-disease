{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (common to many phases - consolidate at the top of your file)\n",
    "import os\n",
    "import sys  # For exiting on errors\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights, vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "# For experiment tracking (choose one - example with Weights & Biases)\n",
    "import wandb\n",
    "\n",
    "# For hyperparameter optimization (example with Optuna)\n",
    "import optuna\n",
    "\n",
    "# For image augmentations (albumentations is very powerful)\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For Grad-CAM\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.cuda.device_count())   # Should print the number of GPUs\n",
    "print(torch.cuda.get_device_name(0)) # Should print your GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/w2sgarnav/vit/runs/tolgnwde?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x72d1b9102fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"vit\", entity=\"w2sgarnav\", name=\"w2sgarnav-vit\", mode=\"online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (Constants and Hyperparameters) ---\n",
    "#  -- Dataset --\n",
    "DATASET_ROOT = \"/home/w2sg-arnav/msusir/SAR-CLD-2024 A Comprehensive Dataset for Cotton Leaf Disease Detection\"  # YOUR DATASET PATH\n",
    "ORIGINAL_DIR = os.path.join(\n",
    "    DATASET_ROOT, \"Original Dataset\"\n",
    ")\n",
    "AUGMENTED_DIR = os.path.join(\n",
    "    DATASET_ROOT, \"Augmented Dataset\"\n",
    ") # Use if you have augmentations\n",
    "\n",
    "CLASSES = [\n",
    "    \"Bacterial Blight\",\n",
    "    \"Curl Virus\",\n",
    "    \"Healthy Leaf\",\n",
    "    \"Herbicide Growth Damage\",\n",
    "    \"Leaf Hopper Jassids\",\n",
    "    \"Leaf Redding\",\n",
    "    \"Leaf Variegation\",\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# -- Training --\n",
    "IMAGE_SIZE = (224, 224)  # Start with 224x224\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4  # Initial learning rate\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 6  # Number of data loading workers (adjust based on your system)\n",
    "VAL_SIZE = 0.2  # Proportion of data for validation\n",
    "TEST_SIZE = 0.2  # Proportion of data for testing\n",
    "RANDOM_STATE = 42  # For reproducibility\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Dataset Class (CottonLeafDataset)\n",
    "\n",
    "class CottonLeafDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading cotton leaf images and labels.\n",
    "    Handles reading images from a directory structure, applying\n",
    "    optional transformations, and providing data to PyTorch's DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, transform=None, class_names=CLASSES):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Path to the root directory of the dataset\n",
    "                           (e.g., ORIGINAL_DIR).\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample (e.g., from torchvision.transforms or\n",
    "                albumentations).\n",
    "            class_names (list): List of class names.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names\n",
    "        self.image_paths, self.labels = self._load_data()\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads image paths and corresponding labels from the dataset directory.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (list of image paths, list of corresponding labels).\n",
    "        \"\"\"\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                print(f\"Warning: Directory not found: {class_dir}\")\n",
    "                continue  # Skip this class if the directory is missing\n",
    "\n",
    "            for image_path in glob.glob(os.path.join(class_dir, \"*.jpg\")) + glob.glob(\n",
    "                os.path.join(class_dir, \"*.jpeg\")) + glob.glob(os.path.join(class_dir, \"*.png\")):\n",
    "                image_paths.append(image_path)\n",
    "                labels.append(i)  # Use numerical labels (0, 1, 2, ...)\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Gets a single sample (image and label) from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label), where image is a PyTorch tensor and\n",
    "                   label is an integer.  Returns a black image and a\n",
    "                   default label if the image can't be loaded.\n",
    "        \"\"\"\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Load image and convert to RGB\n",
    "        except (IOError, FileNotFoundError) as e:\n",
    "            print(f\"Error loading image: {image_path} - {e}. Returning a blank image.\")\n",
    "            # Return a black image and a default label (0)\n",
    "            return torch.zeros((3, *IMAGE_SIZE)), torch.tensor(0)\n",
    "\n",
    "        label = self.labels[idx]  # Get the label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Splitting Function\n",
    "\n",
    "def create_data_splits(data_dir, val_size=0.2, test_size=0.2, random_state=42):\n",
    "    \"\"\"Creates train/validation/test splits stratified by class.\"\"\"\n",
    "    image_files, labels = [], []\n",
    "    for i, class_name in enumerate(CLASSES):\n",
    "      class_path = os.path.join(data_dir, class_name)\n",
    "      for file_path in glob.glob(os.path.join(class_path, \"*\")):\n",
    "          if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):  #check file extension\n",
    "            image_files.append(file_path)\n",
    "            labels.append(i)\n",
    "\n",
    "    # First split: train + temp (for val and test)\n",
    "    train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "        image_files, labels, test_size=(val_size + test_size), random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Second split: temp -> val + test\n",
    "    val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "        temp_files, temp_labels, test_size=(test_size / (val_size + test_size)), random_state=random_state, stratify=temp_labels\n",
    "    )\n",
    "    return train_files, val_files, test_files, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Augmentation and Preprocessing (Transforms)\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_transforms(image_size=IMAGE_SIZE, train=True):\n",
    "    \"\"\"\n",
    "    Defines data augmentation and preprocessing transforms using Albumentations.\n",
    "\n",
    "    Args:\n",
    "        image_size (tuple):  The desired output image size (height, width).\n",
    "        train (bool):  If True, returns transforms for training (including\n",
    "            augmentations). If False, returns transforms for validation/testing\n",
    "            (no augmentations).\n",
    "\n",
    "    Returns:\n",
    "        albumentations.core.composition.Compose:  An Albumentations Compose\n",
    "        object containing the defined transformations.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),\n",
    "            # Augmentations (for training only):\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Normalization (ImageNet statistics):\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),  # Convert to PyTorch tensor\n",
    "        ])\n",
    "    else:\n",
    "        # Validation/Test (no augmentation, just resize and normalize)\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# Custom transform to use PIL and albumentation together.\n",
    "class PILTransform(object):\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img) # PIL -> NumPy\n",
    "        augmented = self.transform(image = img_np) #pass to albumentations\n",
    "        return augmented['image'] #returns a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Image Preprocessing Functions (Revised normalize_orientation)\n",
    "\n",
    "def normalize_orientation(image, mask):\n",
    "    \"\"\"\n",
    "    Normalizes the orientation of the leaf using contour fitting and rotation.\n",
    "    Handles cases where the contour is too small or invalid.\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "    contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:  # Check if any contours were found\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # --- Robustness Checks ---\n",
    "        min_area = 100  # Minimum contour area threshold (adjust as needed)\n",
    "        if cv2.contourArea(largest_contour) < min_area:\n",
    "            print(\"Contour area too small, skipping orientation normalization.\")\n",
    "            return image, mask  # Return original image and mask\n",
    "\n",
    "        if len(largest_contour) < 5:  # fitEllipse needs at least 5 points\n",
    "            print(\"Contour has too few points, skipping orientation normalization.\")\n",
    "            return image, mask\n",
    "\n",
    "        try:\n",
    "            # --- Contour Approximation (Optional, but can help) ---\n",
    "            epsilon = 0.01 * cv2.arcLength(largest_contour, True)  # 1% of perimeter\n",
    "            approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "\n",
    "            # --- Ellipse Fitting and Rotation ---\n",
    "            ellipse = cv2.fitEllipse(approx_contour)  # Use the approximated contour\n",
    "            angle = ellipse[2]\n",
    "            (h, w) = image_np.shape[:2]\n",
    "            center = (w // 2, h // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "            # Use INTER_CUBIC for image, INTER_NEAREST for mask (important!)\n",
    "            rotated_image = cv2.warpAffine(image_np, rotation_matrix, (w, h), flags=cv2.INTER_CUBIC)\n",
    "            rotated_mask = cv2.warpAffine(mask_np, rotation_matrix, (w, h), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "            return Image.fromarray(rotated_image), Image.fromarray(rotated_mask)\n",
    "\n",
    "        except cv2.error as e:\n",
    "            print(f\"OpenCV error during orientation normalization: {e}\")\n",
    "            print(\"Skipping orientation normalization for this image.\")\n",
    "            return image, mask  # Return original image and mask\n",
    "\n",
    "    else:\n",
    "        print(\"No contours found, skipping orientation normalization.\")\n",
    "        return image, mask # Return original image\n",
    "\n",
    "    return image, mask # Return original image if no contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Loader Creation\n",
    "\n",
    "def create_data_loaders(train_files, val_files, test_files, train_labels, val_labels, test_labels, train_transform, val_transform, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    \"\"\"Creates PyTorch DataLoaders for training, validation, and testing.\"\"\"\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = CottonLeafDataset(ORIGINAL_DIR, transform=train_transform)  # Use the full dataset\n",
    "    train_dataset.image_paths = train_files   #then override .image_paths and labels\n",
    "    train_dataset.labels = train_labels\n",
    "\n",
    "    val_dataset = CottonLeafDataset(ORIGINAL_DIR, transform=val_transform)  # Use the full dataset\n",
    "    val_dataset.image_paths = val_files   #then override .image_paths and labels\n",
    "    val_dataset.labels = val_labels\n",
    "\n",
    "\n",
    "    test_dataset = CottonLeafDataset(ORIGINAL_DIR, transform=val_transform)  # Use the full dataset\n",
    "    test_dataset.image_paths = test_files   #then override .image_paths and labels\n",
    "    test_dataset.labels = test_labels\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory = True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1282\n",
      "Number of validation samples: 427\n",
      "Number of test samples: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Data Loading and Preprocessing Pipeline (Putting it all together)\n",
    "\n",
    "# 1. Create data splits\n",
    "train_files, val_files, test_files, train_labels, val_labels, test_labels = create_data_splits(\n",
    "    ORIGINAL_DIR, val_size=VAL_SIZE, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2. Get transforms\n",
    "train_transforms = get_transforms(train=True)\n",
    "val_transforms = get_transforms(train=False)  # Same as test transforms\n",
    "\n",
    "# 3. Apply preprocessing and combine with albumentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    PILTransform(train_transforms), #apply albumentations first\n",
    "    transforms.Lambda(preprocess_image) #then preprocessing\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    PILTransform(val_transforms),  # Use same transforms for val/test\n",
    "    transforms.Lambda(preprocess_image)\n",
    "])\n",
    "# 4. Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "  train_files, val_files, test_files, train_labels, val_labels, test_labels,\n",
    "    train_transforms, val_transforms,\n",
    "    batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_121257/452904154.py\", line 76, in __getitem__\n    image = self.transform(image)  # Apply transformations\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 479, in __call__\n    return self.lambd(img)\n  File \"/tmp/ipykernel_121257/1742540200.py\", line 42, in preprocess_image\n    segmented_image, leaf_mask = segment_leaf(image)\n  File \"/tmp/ipykernel_121257/1742540200.py\", line 6, in segment_leaf\n    image_lab = cv2.cvtColor(image_np, cv2.COLOR_RGB2Lab)\ncv2.error: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-15:Bad number of channels) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3>; VDepth = cv::impl::{anonymous}::Set<0, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize training data (after augmentations and preprocessing)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mvisualize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Data Samples (Augmented & Preprocessed)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Visualize validation data (after preprocessing)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m visualize_data(val_loader, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Data Samples (Preprocessed)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36mvisualize_data\u001b[0;34m(dataloader, num_images, title)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisualize_data\u001b[39m(dataloader, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Samples\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Visualizes a few sample images from a DataLoader.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get a batch of images and labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images):\n",
      "File \u001b[0;32m~/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_121257/452904154.py\", line 76, in __getitem__\n    image = self.transform(image)  # Apply transformations\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/home/w2sg-arnav/anaconda3/envs/cotton_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 479, in __call__\n    return self.lambd(img)\n  File \"/tmp/ipykernel_121257/1742540200.py\", line 42, in preprocess_image\n    segmented_image, leaf_mask = segment_leaf(image)\n  File \"/tmp/ipykernel_121257/1742540200.py\", line 6, in segment_leaf\n    image_lab = cv2.cvtColor(image_np, cv2.COLOR_RGB2Lab)\ncv2.error: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-15:Bad number of channels) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3>; VDepth = cv::impl::{anonymous}::Set<0, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Data Visualization (Optional, but highly recommended)\n",
    "\n",
    "def visualize_data(dataloader, num_images=5, title=\"Data Samples\"):\n",
    "    \"\"\"Visualizes a few sample images from a DataLoader.\"\"\"\n",
    "    images, labels = next(iter(dataloader))  # Get a batch of images and labels\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        # Convert from tensor to numpy array and transpose dimensions\n",
    "        #   PyTorch tensors are (C, H, W), matplotlib expects (H, W, C)\n",
    "        img = images[i].numpy().transpose((1, 2, 0))\n",
    "\n",
    "        # Un-normalize the image (reverse the normalization)\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean  # Un-normalize\n",
    "        img = np.clip(img, 0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title(CLASSES[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training data (after augmentations and preprocessing)\n",
    "visualize_data(train_loader, title=\"Training Data Samples (Augmented & Preprocessed)\")\n",
    "\n",
    "# Visualize validation data (after preprocessing)\n",
    "visualize_data(val_loader, title=\"Validation Data Samples (Preprocessed)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
